{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPXFHtR67XSz"
      },
      "source": [
        "# Introduction to Neural Networks \n",
        "\n",
        "\n",
        "** Ecole Centrale Nantes **\n",
        "\n",
        "** Diana Mateus **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaP5KNXD7XS3"
      },
      "source": [
        "** Participants : **\n",
        "\n",
        "Sakthi Vikneswar Suresh Babu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-FyZE3B7XS4"
      },
      "source": [
        "## General description\n",
        "In this lab we will create a simple classifier based on neural networks. We will progress in two parts:\n",
        "- In the first part, and to better understand the involved operations, we will create a single-neuron model and optimize its parameters \"by hand\". For this first part we will only use the **Numpy** library\n",
        "- We will then build a multi-layer perceptron with the built-in library **Keras** module and **tensorflow**. Tensorflow is already installed in the university computers. If using your own computer you should have already installed **tensorflow** or use **collab** online platform.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg8_j5r97XS5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvCSopBH7XS6"
      },
      "source": [
        "### Loading the dataset\n",
        "Start by runing the following lines to load and visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Uncomment if using COLAB  (and comment next cell)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "IMDIR = ('/content/drive/MyDrive/Colab Notebooks/dataset')\n"
      ],
      "metadata": {
        "id": "7_B5zGllLX9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb_1kDq77XS6"
      },
      "outputs": [],
      "source": [
        "def load_dataset(IMDIR):\n",
        "    train_dataset = h5py.File(IMDIR+'/train_catvnoncat.h5', \"r\")\n",
        "    train_x = np.array(train_dataset[\"train_set_x\"][:]) \n",
        "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
        "    test_dataset = h5py.File(IMDIR+'/test_catvnoncat.h5', \"r\")\n",
        "    test_x = np.array(test_dataset[\"test_set_x\"][:]) \n",
        "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
        "    \n",
        "    train_y = train_y.reshape((1, train_y.shape[0]))\n",
        "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
        "    \n",
        "    return train_x, train_y, test_x, test_y, classes\n",
        "\n",
        "train_x, train_y, test_x, test_y, classes=load_dataset(IMDIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0hXCCuW7XS7"
      },
      "source": [
        "#### Visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcGj2XDE7XS7"
      },
      "outputs": [],
      "source": [
        "# run several times to visualize different data points\n",
        "# the title shows the ground truth class labels (0=no cat , 1 = cat)\n",
        "index = np.random.randint(low=0,high=train_y.shape[1])\n",
        "plt.imshow(train_x[index])\n",
        "plt.title(\"Image \"+str(index)+\" label \"+str(train_y[0,index]))\n",
        "plt.show()\n",
        "print (\"Train X shape: \" + str(train_x.shape))\n",
        "print (\"We have \"+str(train_x.shape[0]), \n",
        "       \"images of dimensionality \" \n",
        "       + str(train_x.shape[1])+ \"x\"\n",
        "       + str(train_x.shape[2])+ \"x\"\n",
        "       + str(train_x.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFV3XzZP7XS8"
      },
      "source": [
        "#### Preprocessing\n",
        "In the following lines we vectorize the images (Instead of a 2-D image we will give as input to the models a 1-D vector). The normalization makes the image intensities be between 0 and 1, and converts the images to floats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SvsLwGb7XS9"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y, classes=load_dataset(IMDIR)\n",
        "train_x = train_x.reshape(train_x.shape[0], -1).T\n",
        "test_x = test_x.reshape(test_x.shape[0], -1).T\n",
        "print (\"Train X shape: \" + str(train_x.shape))\n",
        "print (\"Train Y shape: \" + str(train_y.shape))\n",
        "print (\"Test X shape: \" + str(test_x.shape))\n",
        "print (\"Test Y shape: \" + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuv_0I7O7XS9"
      },
      "outputs": [],
      "source": [
        "train_x = train_x/255.\n",
        "test_x = test_x/255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsHkFIdd7XS-"
      },
      "source": [
        "### 1. Classification with a single neuron \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HI-OoM7XS-"
      },
      "source": [
        "**a)** Fill-in the following three functions to define the single neuron model (a single neuron in the hidden layer):\n",
        "- A function **initialize_parameters** of the neuron. The function will randomly initializes the model's weights with small values. Initialize the bias with 0. What is the number of weights required? pass this information as a parameter to the function.\n",
        "- A function **sigmoid** that computes the sigmoid activation function\n",
        "- A function **neuron** that given an input vector, the weights and bias, computes the output of the single neuron model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_qACoB77XS_"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "#print(sigmoid(0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q4QQDSy7XS_"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(dim):\n",
        "    w = np.random.randn(dim,1)*0.01\n",
        "    b = 0\n",
        "    return w, b\n",
        "print(initialize_parameters(100))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYIgpzVN7XTA"
      },
      "outputs": [],
      "source": [
        "def neuron(w,b,X):\n",
        "    pred_y = sigmoid(np.matmul(w.T,X) + b)\n",
        "    return pred_y\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfaqixLE7XTA"
      },
      "source": [
        "**b)** **Forward Pass:**\n",
        "Use the three functions above to compute a first forward pass for the input matrix $X$ containing the loaded dataset, for some initialization of the weights and bias.\n",
        " \n",
        " \\begin{align}\n",
        " Y_{\\rm pred}=\\sigma(w^\\top X+b) = [y_{\\rm pred}^{(1)},y_{\\rm pred}^{(2)},\\dots,y_{\\rm pred}^{(m)}]\n",
        " \\end{align}\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFWcskKb7XTA"
      },
      "outputs": [],
      "source": [
        "w,b = initialize_parameters(train_x.shape[0])\n",
        "FP = neuron(w,b,train_x)\n",
        "print(FP.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJMQ61FJ7XTB"
      },
      "source": [
        "**c) Cost estimation:**\n",
        " \n",
        "We will use a binary cross-entropy loss, so that the empirical risk can be computed as:\n",
        " \\begin{align}\n",
        " E = - \\frac{1}{m} \\sum_{i=1}^m \n",
        " y^{(i)} \\log(y_{\\rm pred}^{(i)}) +\n",
        " (1-y^{(i)}) \\log(1-y_{\\rm pred}^{(i)})\n",
        " \\end{align}\n",
        " \n",
        " The following cross-entropy function should give as result the scalar cost value computed over the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOfXYhdX7XTB"
      },
      "outputs": [],
      "source": [
        "def crossentropy(Y,Ypred):\n",
        "  m = 209\n",
        "  cost = 0\n",
        "  for i in range (m):\n",
        "    cost = cost + (Y[0,i]*np.log(Ypred[0,i])+(1-Y[0,i])*np.log((1-Ypred[0,i])))\n",
        "  cost1 = cost*(-1/m)\n",
        "  return cost1\n",
        "\n",
        "crossentropy(train_y,FP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpUjfeFl7XTB"
      },
      "source": [
        "**d) Back propagation:**\n",
        "\n",
        "After initializing the parameters and doing a forward pass, we need to backpropagate the cost by computing the gradient with respect to the model parameters to later update the weights\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E}{\\partial w} = \n",
        "& \\frac{1}{m} X(Y_{\\rm pred}-Y)^T = \n",
        " \\frac{1}{m} \\sum_{i=1}^m x^{(i)}(y^{(i)}_{\\rm pred}-y^{(i)})\\\\\n",
        "\\frac{\\partial E}{\\partial b} = \n",
        "& \\frac{1}{m} \\sum_{i=1}^m(y^{(i)}_{\\rm pred}-y^{(i)})\\\\\n",
        "\\end{align}\n",
        "\n",
        "See a demonstration of the gradient computation in \n",
        "https://en.wikipedia.org/wiki/Cross_entropy\n",
        "\n",
        "Fill-in the backpropagation function which receives as input the the training set (X,Y), as well as the current predictions and returns the gradients updates for the weights and bias\n",
        "\n",
        "Hint: When the error is computed for several samples simultaneously, the gradient is averaged over the contribution of different samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnLozAFY7XTC"
      },
      "outputs": [],
      "source": [
        "def backpropagate(X, Y, Ypred):\n",
        "    m = X.shape[1]\n",
        "    dw = 0\n",
        "    db = 0\n",
        "    \n",
        "    #find gradient (back propagation)\n",
        "    dw1 = 1/m*np.matmul(X, (Ypred-Y).T)\n",
        "    db1 = 1/m * np.sum(Ypred-Y)\n",
        "    # for i in range (m):\n",
        "    #   dw = dw + X[0,i]*(Ypred[0,i]-Y[0,i])\n",
        "    # dw1 = dw*(1/m) \n",
        "    # for j in range (m):  \n",
        "    #  db = db + (Ypred[0,i]-Y[0,i])\n",
        "    # db1 = db*(1/m)\n",
        "    grads = {\"dw\": dw1,\n",
        "             \"db\": db1} \n",
        "    \n",
        "    return grads\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYIlVxrX7XTC"
      },
      "source": [
        "**e) Optimization**\n",
        "After initializing the parameters, computing the cost function, and calculating gradients, we can now update the parameters using gradient descent. Use the functions implemented above to fill_in the \"gradient_descent\" function that optimizes the parameters given a training set X, Y, a fixed number of iterations, and a learning_rate. Store and plot the value of the loss function at each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBSR8UOU7XTC"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Y, iterations, learning_rate):\n",
        "    costs = []\n",
        "    w, b = initialize_parameters(train_x.shape[0])\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        Ypred = neuron(w,b,X)\n",
        "        cost = crossentropy(Y, Ypred)\n",
        "        grads= backpropagate(X,Y,Ypred)\n",
        "        \n",
        "        #update parameters\n",
        "        w = w - learning_rate * grads['dw']\n",
        "        b = b - learning_rate * grads['db']\n",
        "        costs.append(cost)\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "       \n",
        "    return w,b, costs\n",
        "\n",
        "w, b, costs = gradient_descent(train_x,train_y,iterations=3000, learning_rate = 0.015)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOfvYu8m7XTD"
      },
      "source": [
        "**e) Plot the training curve**\n",
        "Plot the evolution of the cost vs the iterations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSlvAuw17XTD"
      },
      "outputs": [],
      "source": [
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U0z7L2e7XTE"
      },
      "source": [
        "**f) Prediction**\n",
        "Use the optimized parameters to make predictions both for the train and test sets and compute the accuracy for each. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa5P3G0a7XTE"
      },
      "outputs": [],
      "source": [
        "def predict(w, b, X):    \n",
        "  tmp = neuron(w, b, X)\n",
        "  y_pred = np.zeros((X.shape[1],1))\n",
        "  for i in range(X.shape[1]):\n",
        "    if tmp[0,i] > 0.5:\n",
        "      y_pred[i,0]=1\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "# predict \n",
        "train_pred_y = predict(w, b, train_x)\n",
        "test_pred_y = predict(w, b, test_x)\n",
        "print(\"Train Acc: {} %\".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))\n",
        "print(\"Test Acc: {} %\".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ralVq-xr9kxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf2",
      "language": "python",
      "name": "tf2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}